{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3863f8",
   "metadata": {},
   "source": [
    "This notebook demonstrate the pipeline of training a Graph Neural Network to predict the permeability across Blood Brain Barrier, based on the molecular graphs of small drug molecules. \n",
    "\n",
    "We demonstrate how to run the modules, show some outputs and explain some design choices as markdown text nearby. See the imported modules for the detailed implementation. \n",
    "\n",
    "The 4 sections of the notebook are: \n",
    "\n",
    "1. Curate the process the molecule graphs\n",
    "2. Define and initialize Message Passing Neral Network (MPNN)\n",
    "3. Train the MPNN, inteprete results\n",
    "4. Cluster analysis of the molecules based on training output\n",
    "5. Discussion and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import os\n",
    "\n",
    "# Custom imports (my own modules)\n",
    "from DataLoading import MoleculeGraphProcessor, MPNNDataset     # <-- dataset class\n",
    "from MPNN import MPNNModel                                      # <-- MPNN architecture\n",
    "from Training import MPNNTrainer                                # <-- trainer class\n",
    "from ClusterAnalysis import MoleculeClusterAnalysis             # <-- t-SNE / UMAP class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382a069",
   "metadata": {},
   "source": [
    "1. Curate the process the molecule graphs\n",
    "\n",
    "PAMPA assay is a reliable predictor of blood brain barrier (BBB) permeability for small molecules, (article https://doi.org/10.3389/fphar.2023.1291246). From PubChem database, we obtain a dataset of the PAMPA measurement of 438 small molecules and their SMILES descriptors (PubChem AID: 1845228). \n",
    "\n",
    "We use RDkit to convert the SMILES into molecular graphs, and record 7 node features for each atom, and 4 edge features for each atomi bond. The node features are: atomic number, hybridization, formal charge, aromaticity, number of hydrogen atoms attached and total valence. The edge features are: bond type (as double), conjugated, in ring and aromaticity. \n",
    "\n",
    "For each molecular graph, we also attach the PAMPA permeability measure, as the training target. We print a summary of the dataset after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a98655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder file name containing .csv files of PAMPA assays\n",
    "folder_path = \"PAMPAassays\"  # This folder should be in the same directory as the script\n",
    "\n",
    "# Initialize the processor and process the molecules\n",
    "processor = MoleculeGraphProcessor(folder_path)\n",
    "molecule_graphs, targets = processor.process_molecules()\n",
    "\n",
    "# Create a dataset for training MPNN and save it\n",
    "dataset = MPNNDataset(molecule_graphs, targets)\n",
    "dataset.save(\"PAMPA_dataset.pt\")\n",
    "\n",
    "print(f\"Processed {len(molecule_graphs)} molecules into molecular graphs, and saved PAMPA permiability metric.\\n\")\n",
    "dataset.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b53f0",
   "metadata": {},
   "source": [
    "2. Define and initialize MPNN\n",
    "\n",
    "Message Passing Neural Network is a special type of Graph Neural Networks that utilizes both node features AND edge features, which makes it suitable for differentiating and learning the various chemical bonds in molecular graphs. Message passing describes the activity of passing the imformation of all neighbors of a node to this node, which is an act of aggregation, and similar to convolution. \n",
    "\n",
    "The architecture of our MPNN is as follows:\n",
    "\n",
    "> linear layers for node and edge embedding\n",
    "\n",
    "> MPNN layer 1, containing one MLP(multi layer perceptron) for generating the messages from neighbors, another MLP for updating the node embedding\n",
    "\n",
    "> MPNN layer 2, same weights as MPNN layer 1\n",
    "\n",
    "> MPNN layer 3, same weights as MPNN layer 1\n",
    "\n",
    "> graph level pooling (summarizing all nodes in a graph embedding, we used global_add_pool here)\n",
    "\n",
    "> MLP (a small mlp to reduce graph embedding into 1 output)\n",
    "\n",
    "An MPNN model is initialized in our MPNNModel class. The hidden_dim parameter is the dimension of node and edge embedding, which is a learned high-dimensional vector, describing all the properties of nodes and edges. The num_layers parameter describes how many times message passing happens, and the out_dim parameter is the dimension of output variable, which is the one predicted permeability measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea87c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first need to get the input dimensions of node and edges, i.e. the number of features\n",
    "graph = dataset[0] # read the first graph from dataset \n",
    "\n",
    "node_in_dim = graph.x.shape[1]          # 7 in this case\n",
    "edge_in_dim = graph.edge_attr.shape[1]  # 4 in this case\n",
    "\n",
    "model = MPNNModel(node_in_dim, edge_in_dim, hidden_dim=64, num_layers=3, out_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78bc86",
   "metadata": {},
   "source": [
    "We do a test run of the initialised MPNN by passing one batch of graphs into the model, which generates one batch of permeability prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "model.eval() # run the model in evaluation mode, dropout disabled\n",
    "for batch in loader:\n",
    "    out = model(batch)\n",
    "    print(out.shape)  # [batch_size, 1]\n",
    "    print(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d329acc",
   "metadata": {},
   "source": [
    "3. Train the MPNN, inteprete results\n",
    "\n",
    "A trainer class MPNNTrainer contains all training steps. Here are some technical details: \n",
    "\n",
    "We split the dataset with 70%/15%/15% for the train, validation and test dataset. We train on the train dataset, and check the loss against validation dataset to determine when overfitting occurs, which is the end of training. Test dataset is the held-out set and is used to examine the final model performance. \n",
    "\n",
    "We implement the Huber loss (also known as Smooth L1 Loss) as our loss function. It is designed to combine the best of both Mean Squared Error (MSE) and Mean Absolute Error (MAE). For small errors, it behaves like MSE, giving smooth gradients. For large errors, it behaves like MAE, making it robust to outliers. The Huber_beta parameter corresponds to the threshold of large and small errors. Apart from the Huber loss, we also report a few other metrics on the validation dataset as reference: MSE, RMSE (Root Mean Squared Error) and R^2 (Coefficient of Determination, measures how much of the variance in the true values can be explained by the modelâ€™s predictions).\n",
    "\n",
    "We implement drop out and batch normalisation in the MLPs to improve the robustness and generality of the model. We implement exponential decaying learning rate (lr parameter) as the model approaches the optimal solution. We set early stopping with a patience of 30 epochs to prevent overfitting of the model to train dataset.\n",
    "\n",
    "In the following, we perform training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MPNNTrainer(model, dataset, epochs=150, batch_size=32, lr=5e-4, Huber_beta=2, toy_dataset=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1606ac76",
   "metadata": {},
   "source": [
    "We plot the training historis for train and validation loss, as well as the validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d680173",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da76cb",
   "metadata": {},
   "source": [
    "Finally, after adjusting the hyperparameters and being satisfied with the model, we test the model with held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429dfd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c92a9e",
   "metadata": {},
   "source": [
    "4. Cluster analysis of the molecules based on training output\n",
    "\n",
    "The trained model can return a graph embedding of size [1, hidden_dim], which is a descriptor of the molecular graph. We apply two dimension reduction techniques: tSNE and UMAP, to reduce the size of a graph embedding from a large number (hidden_dim) to 2, and plot that as the x and y coordinates of a point. We label each point with the BBB PAMPA permeability measure to check if the embedding is correlated with the permeability.   \n",
    "\n",
    "We first perform clustering analysis on the whole dataset, and plot the tSNE and UMAP embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e7971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `model` is trained MPNN and dataset is the test dataset from MPNNDataset\n",
    "cluster_analysis = MoleculeClusterAnalysis(model, dataset)\n",
    "\n",
    "# Extract graph embeddings for\n",
    "embeddings = cluster_analysis.extract_embeddings()\n",
    "\n",
    "# Compute 2D projection\n",
    "cluster_analysis.compute_tsne()\n",
    "cluster_analysis.compute_umap()\n",
    "\n",
    "# Plot t-SNE colored by Permeability\n",
    "cluster_analysis.plot_embeddings(color_by='Permeability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57debf86",
   "metadata": {},
   "source": [
    "From the cluster scatter plots, we can see a reasonably obvious permeability gradient over the space, indicating that the graph embedding is corelated with the permeability measure.\n",
    "\n",
    "However, since the model knows the permeability of 70% of the whole dataset, the cluster analysis above is biased towards the training molecules. To assess the generality of the embedding from the trained model, we perform clustering analysis on the test dataset, which the model never saw. \n",
    "\n",
    "Below is an unbiased assessment of the graph embedding, where the high permeability molecules are less separated from the low ones. Mostly, there are many high permeabilties molecules located near the low ones, indicating that more training is needed to learn a better embedding that separates them further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f61ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_analysis = MoleculeClusterAnalysis(model, dataset, test_data=True)\n",
    "embeddings = cluster_analysis.extract_embeddings()\n",
    "cluster_analysis.compute_tsne()\n",
    "cluster_analysis.compute_umap()\n",
    "cluster_analysis.plot_embeddings(color_by='Permeability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc1670",
   "metadata": {},
   "source": [
    "5. Discussion and Next Steps\n",
    "\n",
    "The current model has improved permeability prediction accuracy, evidenced by the reduction of Huber loss and MSE, but the model is not yet good enough as a functional BBB permeability predictor.\n",
    "\n",
    "To improve the MPNN model, we could try incorporate the molecular level features into the computing of the graph embedding. Molecular features such as TPSA (Total Polar Surface Area), molecular weight could be very relevant to BBB permeability. Additionally, we could incorporate the molecular fingerprints from RDKit, which provide substructure information and could be useful if the presence of certain substructures is relevant to BBB Permeability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
